{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7w1Hjrbwu0P"
      },
      "outputs": [],
      "source": [
        "# 基础库\n",
        "!pip install --upgrade transformers datasets peft evaluate qwen-vl-utils\n",
        "\n",
        "\n",
        "\n",
        "# 1) 卸载可能已经装过但元数据不全的版本\n",
        "\n",
        "\n",
        "# Notebook cell\n",
        "!pip uninstall -y bitsandbytes\n",
        "!pip install bitsandbytes       # 会自动匹配 CUDA 版本\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 评估与可视化\n",
        "!pip install scikit-learn matplotlib\n",
        "\n",
        "# 图像处理\n",
        "!pip install pillow\n",
        "!pip install rouge_score\n",
        "!pip install huggingface_hub[hf_xet]\n",
        "!pip install git+https://github.com/salaniz/pycocoevalcap.git\n",
        "!pip install bert_score\n",
        "!pip install --upgrade transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9rK0anCw6R7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from captum.attr import LayerGradCam, LayerAttribution\n",
        "from PIL import Image\n",
        "from datasets import load_dataset\n",
        "\n",
        "# ← Add this\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    Qwen2VLForConditionalGeneration,\n",
        "    BitsAndBytesConfig,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments\n",
        ")\n",
        "import evaluate\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "from bert_score import BERTScorer\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")   # 使用 Agg 后端，不会尝试打开 GUI 窗口\n",
        "import matplotlib.pyplot as plt\n",
        "# ─── 1. Prompt 池 & 随机采样 ───────────────────────────────────────────\n",
        "prompts = [\n",
        "    \"Q: Does this image support the given statement? Explain your reasoning.\",\n",
        "    \"Question: Is the sentence true for the picture shown? Give your reasons.\",\n",
        "    \"Is the description accurate for this image? Explain why or why not.\",\n",
        "    \"Please determine whether the image entails the sentence, and provide a detailed explanation.\",\n",
        "    \"Analyze the image–text pair and explain if they match or conflict.\",\n",
        "    \"Assess whether the statement is consistent with the image, and justify your answer.\",\n",
        "    \"I want you to judge if this sentence correctly describes the image and explain your judgment.\",\n",
        "    \"Your task: verify the sentence against the image and articulate the reasons behind your decision.\"\n",
        "]\n",
        "def sample_prompt():\n",
        "    return random.choice(prompts)\n",
        "\n",
        "# ─── 2. 加载 & 简化数据集 ─────────────────────────────────────────────\n",
        "\n",
        "from datasets import load_dataset# 0. DATASET: 加载 J1mb0o/e-snli-ve\n",
        "raw = load_dataset(\"J1mb0o/e-snli-ve\")\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 只下载并加载前 8 条 train；前 2 条 dev\n",
        "train_ds = (\n",
        "    load_dataset(\"J1mb0o/e-snli-ve\", split=\"train[:8]\")\n",
        "    .rename_column(\"hypothesis\",\"caption\")\n",
        "    .remove_columns([\"gold_label\",\"flickr_id\"])\n",
        ")\n",
        "eval_ds = (\n",
        "    load_dataset(\"J1mb0o/e-snli-ve\", split=\"dev[:2]\")\n",
        "    .rename_column(\"hypothesis\",\"caption\")\n",
        "    .remove_columns([\"gold_label\",\"flickr_id\"])\n",
        ")\n",
        "\n",
        "print(f\"✔️ Train: {len(train_ds)}, Eval: {len(eval_ds)}\")\n",
        "\n",
        "# ─── 3. Processor & Model Setup ─────────────────────────────────────\n",
        "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", use_fast=True)\n",
        "\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,    # 使用半精度\n",
        "    low_cpu_mem_usage=True        # Hugging Face 加速加载权重\n",
        ")\n",
        "\n",
        "peft_cfg = LoraConfig(\n",
        "    r=32, lora_alpha=32, lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
        "    bias=\"none\", task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, peft_cfg)\n",
        "\n",
        "# ─── 4. collate_fn_train: 带 labels ────────────────────────────────────\n",
        "def collate_fn_train(examples):\n",
        "    texts, images = [], []\n",
        "    for ex in examples:\n",
        "        prompt = sample_prompt()\n",
        "        img = ex[\"image\"]\n",
        "        if isinstance(img, dict):\n",
        "            img = Image.open(img[\"path\"])\n",
        "        elif not isinstance(img, Image.Image):\n",
        "            img = Image.open(img)\n",
        "        msgs = [\n",
        "            {\"role\":\"user\",\"content\":[\n",
        "                {\"type\":\"image\",\"image\":img},\n",
        "                {\"type\":\"text\",\"text\":prompt}\n",
        "            ]},\n",
        "            {\"role\":\"assistant\",\"content\":ex[\"caption\"]}\n",
        "        ]\n",
        "        txt = processor.apply_chat_template(\n",
        "            msgs, tokenize=False, add_generation_prompt=False\n",
        "        )\n",
        "        texts.append(txt)\n",
        "        vis, _ = process_vision_info(msgs)\n",
        "        images.append(vis[0])\n",
        "    batch = processor(text=texts, images=images,\n",
        "                      padding=True, return_tensors=\"pt\")\n",
        "    labels = batch.input_ids.clone()\n",
        "    # 把视觉 token & pad token 标记位置的 label 全置 -100\n",
        "    vs = processor.tokenizer.convert_tokens_to_ids(\"<|vision_start|>\")\n",
        "    ve = processor.tokenizer.convert_tokens_to_ids(\"<|vision_end|>\")\n",
        "    vp = processor.tokenizer.convert_tokens_to_ids(\"<|image_pad|>\")\n",
        "    pad = processor.tokenizer.pad_token_id\n",
        "    mask = (labels==vs)|(labels==ve)|(labels==vp)|(labels==pad)\n",
        "    labels[mask] = -100\n",
        "    batch[\"labels\"] = labels\n",
        "    return batch\n",
        "\n",
        "# ─── 5. collate_fn_eval: 只给 prompt+image，用于生成 ────────────────────\n",
        "def collate_fn_eval(examples):\n",
        "    texts, images, captions = [], [], []\n",
        "    for ex in examples:\n",
        "        prompt = sample_prompt()\n",
        "        img = ex[\"image\"]\n",
        "        if isinstance(img, dict):\n",
        "            img = Image.open(img[\"path\"])\n",
        "        elif not isinstance(img, Image.Image):\n",
        "            img = Image.open(img)\n",
        "\n",
        "        msgs = [\n",
        "            {\"role\":\"user\",\"content\":[\n",
        "                {\"type\":\"image\",\"image\":img},\n",
        "                {\"type\":\"text\",\"text\":prompt}\n",
        "            ]}\n",
        "        ]\n",
        "        # add the generation prompt token(s)\n",
        "        txt = processor.apply_chat_template(\n",
        "            msgs, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "        texts.append(txt)\n",
        "        vis, _ = process_vision_info(msgs)\n",
        "        images.append(vis[0])\n",
        "\n",
        "        # **new**: store the reference caption\n",
        "        captions.append(ex[\"caption\"])\n",
        "\n",
        "    batch = processor(\n",
        "        text=texts, images=images,\n",
        "        padding=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    # **new**: keep captions for decoding\n",
        "    batch[\"captions\"] = captions\n",
        "    return batch\n",
        "\n",
        "\n",
        "# ─── 6. compute_metrics_from_texts：纯文本版本指标 ─────────────────────\n",
        "_smooth = SmoothingFunction().method1\n",
        "rouge_metric     = evaluate.load(\"rouge\")\n",
        "bleu_metric      = evaluate.load(\"bleu\")\n",
        "meteor_metric    = evaluate.load(\"meteor\")\n",
        "bertscore_metric = evaluate.load(\"bertscore\")\n",
        "cider_scorer     = Cider()\n",
        "bert_scorer      = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
        "\n",
        "def compute_metrics_from_texts(preds, refs):\n",
        "    # refs 要是 list of list\n",
        "    refs_list = [[r] for r in refs]\n",
        "    r = rouge_metric.compute(predictions=preds, references=refs_list,\n",
        "                              use_stemmer=True)[\"rougeL\"]\n",
        "    b = bleu_metric.compute(predictions=preds, references=refs_list)[\"bleu\"]\n",
        "    m = meteor_metric.compute(predictions=preds, references=refs_list)[\"meteor\"]\n",
        "    c,_ = cider_scorer.compute_score(\n",
        "        {i:[refs[i]] for i in range(len(refs))},\n",
        "        {i:[preds[i]] for i in range(len(preds))}\n",
        "    )\n",
        "    bs = bertscore_metric.compute(predictions=preds, references=refs,\n",
        "                                  lang=\"en\", rescale_with_baseline=True)\n",
        "    f1 = float(sum(bs[\"f1\"]) / len(bs[\"f1\"]))\n",
        "    return r, b, m, c, f1\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import TrainerCallback\n",
        "import torch, evaluate\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "\n",
        "class LowMemEvalCallback(TrainerCallback):\n",
        "    def __init__(\n",
        "        self,\n",
        "        eval_dataset,\n",
        "        collate_fn_eval,\n",
        "        processor,\n",
        "        model,\n",
        "        batch_size: int = 8,\n",
        "        max_eval_samples: int = None\n",
        "    ):\n",
        "        self.processor       = processor\n",
        "        self.model           = model\n",
        "        self.collate_fn_eval = collate_fn_eval\n",
        "\n",
        "        # 只截取前 N 条\n",
        "        if max_eval_samples is not None:\n",
        "            eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
        "\n",
        "        self.loader = DataLoader(\n",
        "            eval_dataset,\n",
        "            batch_size=batch_size,\n",
        "            collate_fn=collate_fn_eval,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        self.rouge  = evaluate.load(\"rouge\")\n",
        "        self.bleu   = evaluate.load(\"bleu\")\n",
        "        self.meteor = evaluate.load(\"meteor\")\n",
        "        self.cider  = Cider()\n",
        "        self.smooth = SmoothingFunction().method1\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        device = self.model.device\n",
        "        all_preds, all_refs = [], []\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in self.loader:\n",
        "                # 1) 移到 device\n",
        "                inputs = {\n",
        "                    k: v.to(device)\n",
        "                    for k, v in batch.items()\n",
        "                    if isinstance(v, torch.Tensor)\n",
        "                }\n",
        "                # 2) 生成\n",
        "                outs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=100,\n",
        "                    num_beams=4,\n",
        "                    early_stopping=True\n",
        "                )\n",
        "                # 3) 解码\n",
        "                #  注意：outs 维度 [B, seq_len]\n",
        "                prefix_len = inputs[\"input_ids\"].shape[-1]\n",
        "                texts = self.processor.batch_decode(\n",
        "                    outs[:, prefix_len:],\n",
        "                    skip_special_tokens=True\n",
        "                )\n",
        "                all_preds.extend(texts)\n",
        "\n",
        "                # 4) 原始 captions 从 batch[\"captions\"] 拿\n",
        "                all_refs.extend(batch[\"captions\"])\n",
        "\n",
        "                # 5) 清理显存\n",
        "                del inputs, outs, texts\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        # 准备 BLEU-1/2 的输入\n",
        "        refs_for_bleu = [[r] for r in all_refs]\n",
        "\n",
        "        rL  = self.rouge.compute(predictions=all_preds, references=all_refs, use_stemmer=True)[\"rougeL\"]\n",
        "        b1 = self.bleu.compute(predictions=all_preds, references=refs_for_bleu, max_order=1)[\"bleu\"]\n",
        "        b2 = self.bleu.compute(predictions=all_preds, references=refs_for_bleu, max_order=2)[\"bleu\"]\n",
        "\n",
        "\n",
        "        m   = self.meteor.compute(predictions=all_preds, references=all_refs)[\"meteor\"]\n",
        "        c,_ = self.cider.compute_score(\n",
        "            {i:[r] for i,r in enumerate(all_refs)},\n",
        "            {i:[p] for i,p in enumerate(all_preds)}\n",
        "        )\n",
        "        print(f\"BLEU-1 {b1:.4f}, BLEU-2 {b2:.4f}\")\n",
        "        print(f\"\\nEpoch {int(state.epoch)} ➜ \"\n",
        "              f\"ROUGE-L {rL:.4f} \"\n",
        "              f\"METEOR {m:.4f}, CIDEr {c:.4f}\\n\")\n",
        "\n",
        "        return control\n",
        "\n",
        "\n",
        "\n",
        "# ─── 7. 训练 & 手动 Eval ─────────────────────────────────────────────\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"output\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=20,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    # —— 日志设置 —— #\n",
        "    report_to=[\"none\"],           # 关掉 WandB，上控制台\n",
        "    logging_strategy=\"steps\",      # 按 step 打印\n",
        "    logging_steps=1,               # 每 1 步输出一次 loss\n",
        "    logging_first_step=True,       # 第 1 步也打印\n",
        "\n",
        "    # —— 自动评估 —— #\n",
        "    eval_strategy=\"epoch\",   # 每个 epoch 跑一次 eval\n",
        "    save_strategy=\"no\",            # 不保存 checkpoint（可改成 \"epoch\"）\n",
        "    predict_with_generate=False,\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    data_collator=collate_fn_train,\n",
        "    callbacks=[\n",
        "      LowMemEvalCallback(\n",
        "        eval_dataset=eval_ds,\n",
        "        collate_fn_eval=collate_fn_eval,\n",
        "        processor=processor,\n",
        "        model=model,\n",
        "        batch_size=4,\n",
        "        max_eval_samples=None\n",
        "      )\n",
        "    ],\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "# 手动生成 + 计算指标\n",
        "pred_out = trainer.predict(\n",
        "    test_dataset=eval_ds,\n",
        "    data_collator=collate_fn_eval,\n",
        "    predict_with_generate=True,\n",
        "    max_length=512,\n",
        "    num_beams=4\n",
        ")\n",
        "logs     = trainer.state.log_history\n",
        "train_ep = [x[\"epoch\"] for x in logs if \"loss\"      in x]\n",
        "train_ls = [x[\"loss\"]  for x in logs if \"loss\"      in x]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(train_ep, train_ls, marker='o', label=\"train_loss\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Train Loss\")\n",
        "plt.legend(); plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def visualize_gradcam(\n",
        "    img: Image.Image,\n",
        "    prompt: str,\n",
        "    model: Qwen2VLForConditionalGeneration,\n",
        "    processor: AutoProcessor,\n",
        "    target_layer: torch.nn.Module,\n",
        "    device: torch.device = None,\n",
        "    upsample_size: tuple = None\n",
        "):\n",
        "    \"\"\"\n",
        "    对 single image+prompt 做 Grad-CAM，可视化目标层的激活热力图叠加。\n",
        "    参数：\n",
        "      img           : PIL.Image，原始输入图\n",
        "      prompt        : str，文字提示\n",
        "      model         : 你的 Qwen2VL 模型\n",
        "      processor     : 对应的 AutoProcessor\n",
        "      target_layer  : 参与 Grad-CAM 的卷积/投影层\n",
        "      device        : torch 设备，默认为 model.device\n",
        "      upsample_size : 热力图放大尺寸，默认为原图大小\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = device or next(model.parameters()).device\n",
        "    upsample_size = upsample_size or img.size[::-1]  # PIL size: (W,H) → numpy( H, W )\n",
        "\n",
        "    # 1) 构造输入\n",
        "    msgs = [{\"role\":\"user\",\"content\":[\n",
        "        {\"type\":\"image\",\"image\":img},\n",
        "        {\"type\":\"text\",\"text\":prompt}\n",
        "    ]}]\n",
        "    pixel_vals, _ = process_vision_info(msgs)\n",
        "    pixel_vals = pixel_vals.unsqueeze(0).to(device)  # [1,3,H,W]\n",
        "    text = processor.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
        "    enc = processor(text=[text], images=[pixel_vals.squeeze(0)],\n",
        "                    return_tensors=\"pt\", padding=True)\n",
        "    enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "    # 2) 定义 Grad-CAM\n",
        "    gradcam = LayerGradCam(model, target_layer)\n",
        "\n",
        "    # 3) 选择一个“目标得分”——这里取模型对第一个生成 token 最大 logit\n",
        "    #    你也可以根据任务选其他目标索引\n",
        "    def score_function(pixel_values, input_ids, attention_mask):\n",
        "        # 前向\n",
        "        out = model(pixel_values=pixel_values,\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask)\n",
        "        logits = out.logits  # [1, seq_len, vocab_size]\n",
        "        last_logits = logits[:, -1, :]  # [1, vocab_size]\n",
        "        # 取概率最高的那个词的 logit 作为目标\n",
        "        target_idx = last_logits.argmax(dim=-1)\n",
        "        return last_logits[:, target_idx]\n",
        "\n",
        "    # 4) 计算归因\n",
        "    attributions = gradcam.attribute(\n",
        "        inputs=(enc[\"pixel_values\"], enc[\"input_ids\"], enc[\"attention_mask\"]),\n",
        "        target=None,\n",
        "        additional_forward_args=(),\n",
        "        attribute_to_layer_input=True,\n",
        "        return_convergence_delta=False,\n",
        "        n_steps=20,\n",
        "        internal_batch_size=1,\n",
        "        **{\"forward_func\": score_function}\n",
        "    )[0]  # 取 pixel_values 那部分，形状 [C,H',W']\n",
        "\n",
        "    # 5) 插值到原图大小\n",
        "    heatmap = LayerAttribution.interpolate(\n",
        "        attributions.cpu().detach(),\n",
        "        upsample_size\n",
        "    ).sum(dim=0).numpy()  # 通道求和 → [H, W]\n",
        "    heatmap = np.maximum(heatmap, 0)\n",
        "    heatmap = heatmap / heatmap.max()\n",
        "\n",
        "    # 6) 可视化\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(img)\n",
        "    plt.imshow(heatmap, cmap=\"jet\", alpha=0.5)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Grad-CAM\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ─── 示例调用 ────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    # 随机从 eval_ds 里拿一张\n",
        "    ex = eval_ds[0]\n",
        "    img = ex[\"image\"]\n",
        "    if not isinstance(img, Image.Image):\n",
        "        img = Image.open(img[\"path\"] if isinstance(img, dict) else img)\n",
        "\n",
        "    prompt = sample_prompt()\n",
        "    # 选一个适合做 Grad-CAM 的层，例如 patch_embed 的投影层\n",
        "    target_layer = model.qwen2vl.visual_encoder.patch_embed.proj\n",
        "\n",
        "    visualize_gradcam(img, prompt, model, processor, target_layer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbBogRELmo9v",
        "outputId": "5fff72cb-fbca-4a1c-976d-1c074f809f03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting captum\n",
            "  Downloading captum-0.8.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from captum) (3.10.0)\n",
            "Collecting numpy<2.0 (from captum)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from captum) (24.2)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.11/dist-packages (from captum) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from captum) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.10->captum)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.10->captum)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.10->captum)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.10->captum)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.10->captum)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.10->captum)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.10->captum)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.10->captum)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.10->captum)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.10->captum)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10->captum) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10->captum) (3.0.2)\n",
            "Downloading captum-0.8.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, captum\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed captum-0.8.0 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "9b0957ddfbae45a7bb547f23ecabeb63",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install captum\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}